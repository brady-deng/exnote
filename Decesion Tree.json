{
  "决策树种类":"CART,ID3,C4.5",
  "总述":"ID3和C4.5生成的决策树可以是多叉的,每个节点下的叉树由该节点特征的取值种类而定.
  而CART假设决策树为二叉树,内部结点特征取值为是与否,左分支取值为是,右分支取值为否,在最后
  用处上,ID3和C4.5都只能用来分类,但是CART既可以用来回归,也可以用来分类",
  "特征选择":"CART分类树通过基尼指数选择最佳特征与最佳切分点,
  ID3首先计算所有特征的信息增益,其中计算的数值是针对每一个特征,不将特征划分,而后以该特
  征将树延伸,在父节点确定的条件下继续对子节点求所有特征的信息增益,后在划分,
  C4.5通过信息增益率选择最优特征,与ID3类似",
  "剪枝":"剪枝也不同,可以参考csdn blog, 决策树CART与ID3,C4.5联系与区别",
  "随机森林":{
    "随机属性选择":"RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练
    过程中引入了随机属性选择，传统决策树在选择划分属性时是在当前节点的属性集合中选择一个
    最优属性，而在RF中，基决策树的每个节点先从该节点的属性集合中随机选择一个包含k个属性
    的子集，然后再从这个子集中选择一个最优属性用于划分，这里的参数k控制了随机性的引入程
    度，若令k=d，则基决策树的构建与传统决策树相同，若令k=1，则随机选择一个属性用于划分，
    一般情况下推荐值k=log(2)(d)",
    "随机森林的随机":"上面所说是随机森林的第一个随机，第二个随机指的是随机且有放回地从
    训练集中抽取N个训练样本（这种采样方式又称为bootstrap sample方法）",
    "特征的选择":"如果每个样本的特征维度为M，指定一个常数m《M，随机地从M个特征中选取m个
    特征子集，每次树进行分裂时，从这m个特征中选择最优的。",
    "分类效果":"随机森林的分类效果与两个因素有关：森林中任意两棵树的相关性，相关性越大，
    错误率越大。森林中每棵树的分类能力，每棵树的分类能力越强，整个森林的错误率越低。",
    "随机森林的参数":"减小特征选择个数m，树的相关性和分类能力也会相应的降低，增大m，两
    者也会随之增大，所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一
    个参数",
    "袋外错误率":"out-of-bag error，在构建每棵树的时候，我们对训练集使用了不同的
    bootstrap sample，所以对于每棵树而言，大约有1/3的训练实例没有参与第k棵树的生成，它
    们成为第k棵树的oob样本"
  }
}
