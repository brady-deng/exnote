"Decision Trees(DTs)":
{
    "definition":"a non-parametric supervised learning method used for
  classification and regression.",
    "some advantages":{
      "1":"Requires little data preparation",
      "2":"The cost of using the tree is logarithmic in the number of
      data points used to train the tree",
      "3":"Uses a white box model."
    },
    "some disadvantages":{
      "1":"Decision-tree learners can create over-complex trees that do not
      generalise the data well. This is called overfitting. Mechanisms such
      as pruning, setting the minimum number of samples required at a leaf node
      or setting the maximum depth of the tree are necessary to avoid this problem",
      "2":"Decision trees can be unstable because small variations in the
      data might result in a completely different tree being generated.
      This problem is mitigated by using decision trees within an ensemble.",
      "3":"practical decision-tree learning algorithms are based on heuristic
      algorithms such as the greedy algorithm where locally optimal decisions
      are made at each node. Such algorithms cannot guarantee to return the
      globally optimal decision tree. This can be mitigated by training multiple
      trees in an ensemble learner.",
      "4":"Decision tree learners create biased trees if some classes dominate.
      It is therefore recommended to balance the dataset prior to fitting with the
      decision tree."
    },
    "some Tips":{
      "1":"Decision trees tend to overfit on data with a large number of features.
      Getting the right ratio of samples to numbers of features is important, since
      a tree with few samples in high dimensional space is very likely to overfit",
      "2":"Visualise your tree as you are training by using the export function.
      Use max_depth=3 as an initial tree depth to get a feel for how the tree is
      fitting to your data, and then increase the depth.",
      "3":"Remember that the number of samples required to populate the tree
      doubles for each additional level the tree grows to. Use max_depth to
      control the size of the tree to prevent overfitting.",
      "4":"Use min_samples_split or min_samples_leaf to control the number of
      samples at a leaf node. A very small number will usually mean the tree
      will overfit, whereas a large number will prevent the tree from learning
      the data. Try min_samples_leaf=5 as an initial value. If the sample size
      varies greatly, a float number can be used as percentage in these two
      parameters. The main difference between the two is that min_samples_leaf
      guarantees a minimum number of samples in a leaf, while min_samples_split
      can create arbitrary small leaves, though min_samples_split is more common
       in the literature.",
      "5":"Balance your dataset before training to prevent the tree from being
      biased toward the classes that are dominant. Class balancing can be done
      by sampling an equal number of samples from each class, or preferably by
      normalizing the sum of the sample weights (sample_weight) for each class
      to the same value. Also note that weight-based pre-pruning criteria, such
      as min_weight_fraction_leaf, will then be less biased toward dominant
      classes than criteria that are not aware of the sample weights, like
      min_samples_leaf.",
    },
    "Tree algorithms":{
      "ID3":"ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.
       The algorithm creates a multiway tree, finding for each node (i.e. in a
       greedy manner) the categorical feature that will yield the largest information
       gain for categorical targets. Trees are grown to their maximum size and then
       a pruning step is usually applied to improve the ability of the tree to
       generalise to unseen data.",
       "C4.5":"C4.5 is the successor to ID3 and removed the restriction that features
       must be categorical by dynamically defining a discrete attribute (based on
       numerical variables) that partitions the continuous attribute value into a
       discrete set of intervals. C4.5 converts the trained trees (i.e. the output
       of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule
       is then evaluated to determine the order in which they should be applied.
       Pruning is done by removing a rule’s precondition if the accuracy of the
       rule improves without it.",
       "C5.0":"C5.0 is Quinlan’s latest version release under a proprietary license.
       It uses less memory and builds smaller rulesets than C4.5 while being more
       accurate.",
       "CART":"CART (Classification and Regression Trees) is very similar to C4.5,
       but it differs in that it supports numerical target variables (regression)
       and does not compute rule sets. CART constructs binary trees using the feature
       and threshold that yield the largest information gain at each node."
    }


}
"Something should to know":
{
  "data normalisation":"数据标准化",
  "discrete data":"离散型数据",
  "continuous data":"连续性数据",
  "numerical data":"具有实际测量的物理意义，比如说人的身高、体重、IQ、血压等等",
  "categorical data":"代表了被描述对象的性质，比如说一个人的性别、婚姻状况、家乡等，
  也可以称作是qualitative data或者是Yes/No data",
  "heuristic algorithms":"启发式算法",
  "greedy algorithms":"贪心算法",
  "NP-completeness":"NP完全问题，待补充...",
  "XOR":"exclusive OR, 缩写成xor异或逻辑运算，如果输入两个值不相同，则异或结果为1，
  如果两个值相同，异或结果为0",
  "Parity":"奇偶校验位",
  "Multiplexer":"多路复用器",
  "discriminative":"区别的，歧视的，有识别能力的",
  "prune":"剪枝",
  "dominant":"统治的",
}
"Some good songs":
{
  "1":{
    "name":"Falling Slowly",
    "singer":"Glen Hensord"
  },
  "2":{

  }
}
