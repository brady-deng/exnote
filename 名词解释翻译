"Decision Trees(DTs)":
{
    "definition":"a non-parametric supervised learning method used for
  classification and regression.",
    "some advantages":{
      "1":"Requires little data preparation",
      "2":"The cost of using the tree is logarithmic in the number of
      data points used to train the tree",
      "3":"Uses a white box model."
    },
    "some disadvantages":{
      "1":"Decision-tree learners can create over-complex trees that do not
      generalise the data well. This is called overfitting. Mechanisms such
      as pruning, setting the minimum number of samples required at a leaf node
      or setting the maximum depth of the tree are necessary to avoid this problem",
      "2":"Decision trees can be unstable because small variations in the
      data might result in a completely different tree being generated.
      This problem is mitigated by using decision trees within an ensemble.",
      "3":"practical decision-tree learning algorithms are based on heuristic
      algorithms such as the greedy algorithm where locally optimal decisions
      are made at each node. Such algorithms cannot guarantee to return the
      globally optimal decision tree. This can be mitigated by training multiple
      trees in an ensemble learner.",
      "4":"Decision tree learners create biased trees if some classes dominate.
      It is therefore recommended to balance the dataset prior to fitting with the
      decision tree."
    },
    "some Tips":{
      "1":"Decision trees tend to overfit on data with a large number of features.
      Getting the right ratio of samples to numbers of features is important, since
      a tree with few samples in high dimensional space is very likely to overfit",
      "2":"Visualise your tree as you are training by using the export function.
      Use max_depth=3 as an initial tree depth to get a feel for how the tree is
      fitting to your data, and then increase the depth.",
      "3":"Remember that the number of samples required to populate the tree
      doubles for each additional level the tree grows to. Use max_depth to
      control the size of the tree to prevent overfitting.",
      "4":"Use min_samples_split or min_samples_leaf to control the number of
      samples at a leaf node. A very small number will usually mean the tree
      will overfit, whereas a large number will prevent the tree from learning
      the data. Try min_samples_leaf=5 as an initial value. If the sample size
      varies greatly, a float number can be used as percentage in these two
      parameters. The main difference between the two is that min_samples_leaf
      guarantees a minimum number of samples in a leaf, while min_samples_split
      can create arbitrary small leaves, though min_samples_split is more common
       in the literature.",
      "5":"Balance your dataset before training to prevent the tree from being
      biased toward the classes that are dominant. Class balancing can be done
      by sampling an equal number of samples from each class, or preferably by
      normalizing the sum of the sample weights (sample_weight) for each class
      to the same value. Also note that weight-based pre-pruning criteria, such
      as min_weight_fraction_leaf, will then be less biased toward dominant
      classes than criteria that are not aware of the sample weights, like
      min_samples_leaf.",
    },
    "Tree algorithms":{
      "ID3":"ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.
       The algorithm creates a multiway tree, finding for each node (i.e. in a
       greedy manner) the categorical feature that will yield the largest information
       gain for categorical targets. Trees are grown to their maximum size and then
       a pruning step is usually applied to improve the ability of the tree to
       generalise to unseen data.",
       "C4.5":"C4.5 is the successor to ID3 and removed the restriction that features
       must be categorical by dynamically defining a discrete attribute (based on
       numerical variables) that partitions the continuous attribute value into a
       discrete set of intervals. C4.5 converts the trained trees (i.e. the output
       of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule
       is then evaluated to determine the order in which they should be applied.
       Pruning is done by removing a rule’s precondition if the accuracy of the
       rule improves without it.",
       "C5.0":"C5.0 is Quinlan’s latest version release under a proprietary license.
       It uses less memory and builds smaller rulesets than C4.5 while being more
       accurate.",
       "CART":"CART (Classification and Regression Trees) is very similar to C4.5,
       but it differs in that it supports numerical target variables (regression)
       and does not compute rule sets. CART constructs binary trees using the feature
       and threshold that yield the largest information gain at each node."
    }


}
"Ebsemble methods":{
  "综述":"目前的集成学习方法大致可以分为两大类，即个体学习器之间存在强依赖关系，必须串行生成的
  序列化方法（Boosting），以及个体学习器间不存在强依赖关系】可同时生成的并行化方法（Bagging）",

  "sklearn中的Ensemble methods":{
    "1":"Bagging meta-estimator(Bagging classifier)",
    "2":"Random Forests(RandomForestClassifier)",
    "3":"Extremely Randomized Trees(ExtraTreesClassifier)",
    "4":"Gradient Tree Boosting(GradientBoostingClassifier)",
    "5":"Voting Classifier(VotingClassifier)",
  },
  "1":{
    "Bagging":"bagging,即套袋法，bagging方法是bootstrap aggregating的缩写，采用的是随机有放回的选择训练数据然
    后构造分类器，最后组合的方法。In ensemble algorithms, bagging methods form a class of
    algorithms which build several instances of a black-box estimator on random subsets
    of the original training set and then aggregate their individual predictions to form
    a final prediction. These methods are used as a way to reduce the variance of a base
    estimator (e.g., a decision tree), by introducing randomization into its construction
    procedure and then making an ensemble out of it. In many cases, bagging methods
    constitute a very simple way to improve with respect to a single model, without making
    it necessary to adapt the underlying base algorithm. As they provide a way to reduce
    overfitting, bagging methods work best with strong and complex models (e.g., fully
    developed decision trees), in contrast with boosting methods which usually work best
    with weak models (e.g., shallow decision trees)",
    "过程":"A）从原始样本集中抽取训练集.每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本
    （在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）.共进行k轮抽取，得到k
    个训练集.（k个训练集相互独立）
    B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型.（注：根据具体问题采用不同的分类或回
    归方法，如决策树、神经网络等）
    C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为
    最后的结果. ",
    "procedure":"When random subsets of the dataset are drawn as random subsets of the samples,
    then this algorithm is known as Pasting [B1999].
    When samples are drawn with replacement, then the method is known as Bagging [B1996].
    When random subsets of the dataset are drawn as random subsets of the features, then
    the method is known as Random Subspaces [H1998].
    Finally, when base estimators are built on subsets of both samples and features, then
    the method is known as Random Patches [LG2012].",
    "sklearn bagging classifier":"A Bagging classifier is an ensemble meta-estimator that fits
    base classifiers each on random subsets of the original dataset and then aggregate their
    individual predictions (either by voting or by averaging) to form a final prediction.
    Such a meta-estimator can typically be used as a way to reduce the variance of a
    black-box estimator (e.g., a decision tree), by introducing randomization into its
    construction procedure and then making an ensemble out of it.",
  },
  "3":{
    "boosting":"Boosting是一族可将弱学习器提升为强学习器的算法.关于Boosting的两个核心问题：
    1）在每一轮如何改变训练数据的权值或概率分布？
    通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样本的权值，而误分的样本在后续受到
    更多的关注.
    2）通过什么方式来组合弱分类器？
    通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器
    的权值，同时减小错误率较大的分类器的权值.
    而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型.",
  },
  "4":{
    "bagging 与 boosting区别":"1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中
    选出的各轮训练集之间是独立的.Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重
    发生变化.而权值是根据上一轮的分类结果进行调整.
    2）样例权重：
    Bagging：使用均匀取样，每个样例的权重相等
    Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.
    3）预测函数：
    Bagging：所有预测函数的权重相等.
    Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.
    4）并行计算：
    Bagging：各个预测函数可以并行生成
    Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果."
  },
  "5":{
    "随机森林":"随机森林是Bagging的一个扩展变体，除了样本集是有放回的采样外，属性集合也引入了随机属性
    选择.具体来说，传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在RF中，对基
    决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个
    最优属性用于划分. 随机森林简单、容易实现、计算开销小.效果能使得最终集成的泛化性能可通过个体学习器
    之间差异度的增加而进一步提升."
  },
  "6":{
    "Voting classifier":"The idea behind the VotingClassifier is to combine conceptually
    different machine learning classifiers and use a majority vote or the average predicted
    probabilities (soft vote) to predict the class labels. Such a classifier can be useful
    for a set of equally well performing model in order to balance out their individual
    weaknesses.",
    "Majority Class Labels(Majority/Hard Voting)":"In majority voting, the predicted class
    label for a particular sample is the class label that represents the majority (mode)
    of the class labels predicted by each individual classifier.",
    "Weighted Average probabilities(Soft Voting)":"In contrast to majority voting (hard
    voting), soft voting returns the class label as argmax of the sum of predicted
    probabilities.Specific weights can be assigned to each classifier via the weights
    parameter. When weights are provided, the predicted class probabilities for each
    classifier are collected, multiplied by the classifier weight, and averaged. The
    final class label is then derived from the class label with the highest average
    probability."
  }

}
"Something should to know":
{
  "data normalisation":"数据标准化",
  "discrete data":"离散型数据",
  "continuous data":"连续性数据",
  "numerical data":"具有实际测量的物理意义，比如说人的身高、体重、IQ、血压等等",
  "categorical data":"代表了被描述对象的性质，比如说一个人的性别、婚姻状况、家乡等，
  也可以称作是qualitative data或者是Yes/No data",
  "heuristic algorithms":"启发式算法",
  "greedy algorithms":"贪心算法",
  "NP-completeness":"NP完全问题，待补充...",
  "XOR":"exclusive OR, 缩写成xor异或逻辑运算，如果输入两个值不相同，则异或结果为1，
  如果两个值相同，异或结果为0",
  "Parity":"奇偶校验位",
  "Multiplexer":"多路复用器",
  "discriminative":"区别的，歧视的，有识别能力的",
  "prune":"剪枝",
  "dominant":"统治的",
}
"Some good songs":
{
  "1":{
    "name":"Falling Slowly",
    "singer":"Glen Hensord"
  },
  "2":{

  }
}
