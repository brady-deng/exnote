"Decision Trees(DTs)":
{
    "definition":"a non-parametric supervised learning method used for
  classification and regression.",
    "some advantages":{
      "1":"Requires little data preparation",
      "2":"The cost of using the tree is logarithmic in the number of
      data points used to train the tree",
      "3":"Uses a white box model."
    },
    "some disadvantages":{
      "1":"Decision-tree learners can create over-complex trees that do not
      generalise the data well. This is called overfitting. Mechanisms such
      as pruning, setting the minimum number of samples required at a leaf node
      or setting the maximum depth of the tree are necessary to avoid this problem",
      "2":"Decision trees can be unstable because small variations in the
      data might result in a completely different tree being generated.
      This problem is mitigated by using decision trees within an ensemble.",
      "3":"practical decision-tree learning algorithms are based on heuristic
      algorithms such as the greedy algorithm where locally optimal decisions
      are made at each node. Such algorithms cannot guarantee to return the
      globally optimal decision tree. This can be mitigated by training multiple
      trees in an ensemble learner.",
      "4":"Decision tree learners create biased trees if some classes dominate.
      It is therefore recommended to balance the dataset prior to fitting with the
      decision tree."
    },
    "some Tips":{
      "1":"Decision trees tend to overfit on data with a large number of features.
      Getting the right ratio of samples to numbers of features is important, since
      a tree with few samples in high dimensional space is very likely to overfit",
      "2":"Visualise your tree as you are training by using the export function.
      Use max_depth=3 as an initial tree depth to get a feel for how the tree is
      fitting to your data, and then increase the depth.",
      "3":"Remember that the number of samples required to populate the tree
      doubles for each additional level the tree grows to. Use max_depth to
      control the size of the tree to prevent overfitting.",
      "4":"Use min_samples_split or min_samples_leaf to control the number of
      samples at a leaf node. A very small number will usually mean the tree
      will overfit, whereas a large number will prevent the tree from learning
      the data. Try min_samples_leaf=5 as an initial value. If the sample size
      varies greatly, a float number can be used as percentage in these two
      parameters. The main difference between the two is that min_samples_leaf
      guarantees a minimum number of samples in a leaf, while min_samples_split
      can create arbitrary small leaves, though min_samples_split is more common
       in the literature.",
      "5":"Balance your dataset before training to prevent the tree from being
      biased toward the classes that are dominant. Class balancing can be done
      by sampling an equal number of samples from each class, or preferably by
      normalizing the sum of the sample weights (sample_weight) for each class
      to the same value. Also note that weight-based pre-pruning criteria, such
      as min_weight_fraction_leaf, will then be less biased toward dominant
      classes than criteria that are not aware of the sample weights, like
      min_samples_leaf.",
    },
    "Tree algorithms":{
      "ID3":"ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.
       The algorithm creates a multiway tree, finding for each node (i.e. in a
       greedy manner) the categorical feature that will yield the largest information
       gain for categorical targets. Trees are grown to their maximum size and then
       a pruning step is usually applied to improve the ability of the tree to
       generalise to unseen data.",
       "C4.5":"C4.5 is the successor to ID3 and removed the restriction that features
       must be categorical by dynamically defining a discrete attribute (based on
       numerical variables) that partitions the continuous attribute value into a
       discrete set of intervals. C4.5 converts the trained trees (i.e. the output
       of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule
       is then evaluated to determine the order in which they should be applied.
       Pruning is done by removing a rule’s precondition if the accuracy of the
       rule improves without it.",
       "C5.0":"C5.0 is Quinlan’s latest version release under a proprietary license.
       It uses less memory and builds smaller rulesets than C4.5 while being more
       accurate.",
       "CART":"CART (Classification and Regression Trees) is very similar to C4.5,
       but it differs in that it supports numerical target variables (regression)
       and does not compute rule sets. CART constructs binary trees using the feature
       and threshold that yield the largest information gain at each node."
    }


}
"Ebsemble methods":{
  "综述":"目前的集成学习方法大致可以分为两大类，即个体学习器之间存在强依赖关系，必须串行生成的
  序列化方法（Boosting），以及个体学习器间不存在强依赖关系】可同时生成的并行化方法（Bagging）",

  "sklearn中的Ensemble methods":{
    "1":"Bagging meta-estimator(Bagging classifier)",
    "2":"Random Forests(RandomForestClassifier)",
    "3":"Extremely Randomized Trees(ExtraTreesClassifier)",
    "4":"Gradient Tree Boosting(GradientBoostingClassifier)",
    "5":"Voting Classifier(VotingClassifier)",
  },
  "1":{
    "Bagging":"bagging,即套袋法，bagging方法是bootstrap aggregating的缩写，采用的是随机有放回的选择训练数据然
    后构造分类器，最后组合的方法。In ensemble algorithms, bagging methods form a class of
    algorithms which build several instances of a black-box estimator on random subsets
    of the original training set and then aggregate their individual predictions to form
    a final prediction. These methods are used as a way to reduce the variance of a base
    estimator (e.g., a decision tree), by introducing randomization into its construction
    procedure and then making an ensemble out of it. In many cases, bagging methods
    constitute a very simple way to improve with respect to a single model, without making
    it necessary to adapt the underlying base algorithm. As they provide a way to reduce
    overfitting, bagging methods work best with strong and complex models (e.g., fully
    developed decision trees), in contrast with boosting methods which usually work best
    with weak models (e.g., shallow decision trees)",
    "过程":"A）从原始样本集中抽取训练集.每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本
    （在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）.共进行k轮抽取，得到k
    个训练集.（k个训练集相互独立）
    B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型.（注：根据具体问题采用不同的分类或回
    归方法，如决策树、神经网络等）
    C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为
    最后的结果. ",
    "procedure":"When random subsets of the dataset are drawn as random subsets of the samples,
    then this algorithm is known as Pasting [B1999].
    When samples are drawn with replacement, then the method is known as Bagging [B1996].
    When random subsets of the dataset are drawn as random subsets of the features, then
    the method is known as Random Subspaces [H1998].
    Finally, when base estimators are built on subsets of both samples and features, then
    the method is known as Random Patches [LG2012].",
    "sklearn bagging classifier":"A Bagging classifier is an ensemble meta-estimator that fits
    base classifiers each on random subsets of the original dataset and then aggregate their
    individual predictions (either by voting or by averaging) to form a final prediction.
    Such a meta-estimator can typically be used as a way to reduce the variance of a
    black-box estimator (e.g., a decision tree), by introducing randomization into its
    construction procedure and then making an ensemble out of it.",
  },
  "sup":{
    "max_features":"
    Increasing max_features generally improves the performance of the model as at
    each node now we have a higher number of options to be considered. However,
    this is not necessarily true as this decrease the diversity of individual tree
    which is the USP of random forest, but, for sure, you decrease the speed of
    algorithm by increasing the max_features. Hence, you need to strike the right
    balance and choose the optimal max_features.
    ",
    "n_estimators":"
    this is the number of trees you want to build before taking the maximum voting
    or averages of predictions. Higher number of trees give you better performance
    but makes your code slowere. You should choose as high value as your processor
    can handle because this makes your predictions stronger and more stable
    ",
    "min_sample_leaf":"
    If you have built a decision tree before, you can appreciate the importance
    of minimum sample leaf size. Leaf is the end node of a decision tree. A smaller
     leaf makes the model more prone to capturing noise in train data. Generally
      I prefer a minimum leaf size of more than 50. However, you should try multiple
       leaf sizes to find the most optimum for your use case.
    "
  },
  "3":{
    "boosting":"Boosting是一族可将弱学习器提升为强学习器的算法.关于Boosting的两个核心问题：
    1）在每一轮如何改变训练数据的权值或概率分布？
    通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样本的权值，而误分的样本在后续受到
    更多的关注.
    2）通过什么方式来组合弱分类器？
    通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器
    的权值，同时减小错误率较大的分类器的权值.
    而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型.",
  },
  "4":{
    "bagging 与 boosting区别":"1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中
    选出的各轮训练集之间是独立的.Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重
    发生变化.而权值是根据上一轮的分类结果进行调整.
    2）样例权重：
    Bagging：使用均匀取样，每个样例的权重相等
    Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.
    3）预测函数：
    Bagging：所有预测函数的权重相等.
    Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.
    4）并行计算：
    Bagging：各个预测函数可以并行生成
    Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果."
  },
  "5":{
    "随机森林":"随机森林是Bagging的一个扩展变体，除了样本集是有放回的采样外，属性集合也引入了随机属性
    选择.具体来说，传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在RF中，对基
    决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个
    最优属性用于划分. 随机森林简单、容易实现、计算开销小.效果能使得最终集成的泛化性能可通过个体学习器
    之间差异度的增加而进一步提升."
  },
  "6":{
    "Voting classifier":"The idea behind the VotingClassifier is to combine conceptually
    different machine learning classifiers and use a majority vote or the average predicted
    probabilities (soft vote) to predict the class labels. Such a classifier can be useful
    for a set of equally well performing model in order to balance out their individual
    weaknesses.",
    "Majority Class Labels(Majority/Hard Voting)":"In majority voting, the predicted class
    label for a particular sample is the class label that represents the majority (mode)
    of the class labels predicted by each individual classifier.",
    "Weighted Average probabilities(Soft Voting)":"In contrast to majority voting (hard
    voting), soft voting returns the class label as argmax of the sum of predicted
    probabilities.Specific weights can be assigned to each classifier via the weights
    parameter. When weights are provided, the predicted class probabilities for each
    classifier are collected, multiplied by the classifier weight, and averaged. The
    final class label is then derived from the class label with the highest average
    probability."
  }

}
"Something should to know":
{
  "data normalisation":"数据标准化",
  "discrete data":"离散型数据",
  "continuous data":"连续性数据",
  "numerical data":"具有实际测量的物理意义，比如说人的身高、体重、IQ、血压等等",
  "categorical data":"代表了被描述对象的性质，比如说一个人的性别、婚姻状况、家乡等，
  也可以称作是qualitative data或者是Yes/No data",
  "heuristic algorithms":"启发式算法",
  "greedy algorithms":"贪心算法",
  "NP-completeness":"NP完全问题，待补充...",
  "XOR":"exclusive OR, 缩写成xor异或逻辑运算，如果输入两个值不相同，则异或结果为1，
  如果两个值相同，异或结果为0",
  "Parity":"奇偶校验位",
  "Multiplexer":"多路复用器",
  "discriminative":"区别的，歧视的，有识别能力的",
  "prune":"剪枝",
  "dominant":"统治的",
  "RDBMS":"Relational Database Management System, 关系数据库管理系统",
  "BSON":"binary Serialized Document Format, 是一种二进制形式的存储格式，是一种类
  json的二进制形式的存储格式，简称Binary JSON",
  "Ad-Hoc":"和以前的直连双绞线概念一样，是P2P的链接，所以也就无法与其他网络沟通了。",
  "AD-Hoc查询":"即席查询，是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的
  选择生成相应的统计报表，即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，
  而即席查询是由用户自定义查询条件的。",
  "slug":"偷懒",
  "数据库中的多对多关系":"在RDMBS中，我们可以使用交叉表来表示多对多的关系，交叉表把多对
  多的关系存储在单个表里，使用SQL join语句就可以查询与商品相关的所有类别，反之亦然.
  MongoDB不支持join连接，所以需要不同的多对多策略。",
  "Rollback":"回滚，指的是程序或数据处理错误，将程序或者数据恢复到上一次正确状态的行为。
  回滚包括程序回滚和数据回滚等类型。",
  "集合":"集合是结构上或概念上相似的文档的容器，MongoDB中创建集合是隐式创建的，也有一个
  创建集合的命令：db.createCollection("",{size:****});除了创建上述的标准集合意外还可
  以创建盖子集合（capped collection），或者称为是有上限的集合，一旦盖子集合达到最大上限
  后续的插入将会覆盖最先插入的文档数据，除了大小限制意外，MongoDB允许为盖子集合指定最大
  文档数量的max参数，这允许对存储文档的总数进行细粒度控制。大小配置具备优先权，创建集合
  的方式如下所示：
  db.createCollection('users.actions',{capped:true,size:16384,max:100}),
  盖子集合不允许正常集合的所有操作，例如不允许从盖子集合中删除单个文档，也不能增加文档的大小",
  "TTL集合":"MongoDB也允许在特定的时间后废弃文档数据，有时候叫做生存时间time-to-live
  （TTL）集合，创建TTl索引的方式如下：
  db.reviews.createIndex({time_field:1},{expireAfterSeconds:3600}),
  这个命令会在（time_field）字段上创建索引，这个字段会定期检查时间戳，与当前的时间值比较
  如果time_field与当前时间值的差距大于expireAfterSeconds设置，文档就会自动被删除，这个
  例子里评价文档会在一个小时后删除。",
  "系统集合":"MongoDB有两个特殊的集合：system.namespaces和system.indexes.我们可以使用
  前者查询当前数据库定义的所有命名空间，后者存储了当前数据库里面定义的所有索引",
  "PPV":"positive predictive value, 和精准率的计算公式一样",
  "NPV":"negative predictive value",
  "Cohen's kappa coefficient":"是一种一致性检验的量化指标，由以下公式表示：
  k = (p0-pe)/(1-pe) = 1-(1-p0)/(1-pe),p0就是正确率，pe的计算比较复杂，如果两种分布
  完全相同，k=1，如果完全不同，则k=0.kappa大于0.75表示好的一致性，小于0.4表示一致性差",
  "baseline wander":"基线漂移",
  "power noise":"功率噪声，电源噪声",
  "PRV":"pulse rate variability",
  "spike noise":"尖峰噪声",
  "ODI":"the total sum of oxygen desaturation events per hour while sleep/wake
  states were estimated using the PPG, 也就是每小时血氧欠饱和次数",
  "oxygen desaturation event":"a region with a decrease of more than 3% below the
  baseline during sleep or a decrease of more than 4% below the baseline during
  wake state, according to the sleep/wake states. 睡眠的时候血氧下降超过3%或者清醒的
  时候血氧下降超过4%",
  "Bland-Altman plot":"布兰德-奥特曼图，a bland-altman plot in analytical chemistry
  or biomedicine is a method of data plotting used in analyzing the agreement be-
  tween two different assays. 也可以叫做Tukey mean-difference plot,横轴是两种测量方
  法的均值，纵轴是两种测量方法的差值，bland-altman图法主要观察两种测量之间差异的分布，在
  纵轴上以差异的均值和理论0值为均值参考线，另外再添加md+-1.96SD即差异均值的95%的置信区间
  的参考线，也称为95%的可接受的一致性界限，报告结果需要结合图形做合理解释，一般情况下报告
  在一致性界限外的点的百分比，与理论值相差的最大值，结合临床，进行合理解释，决定对新方法的
  取舍",
  "assay":"化验，试验",
  "matlab中的nargin":"nargin是用来判断输入变量个数的函数",
  "two-feature strong classifier":"两维特征强分类器",
  "nevertheless":"然而，不过，虽然如此",
  "schematic":"原理图",
  "eliminate":"排除，消除",
  "subsequent":"后来的，随后的",
  "drawback":"缺点，不利条件",
  "diversity":"多样性，差异",
  "prone":"更易于，prone to do something",
  "progress down":"进步了",
  "feasibility":"可行性",
  "impractically":"不切实际的",



}
"Some good songs":
{
  "1":{
    "name":"Falling Slowly",
    "singer":"Glen Hensord"
  },
  "2":{

  }
},
"文献写法，有关级联分类器":
{
"1":"
This section describes an algorithm for constructing a cascade of classifiers which
achieves increased detection performance while radically reducing computation time.
cascade,小瀑布，串联
achieve,取得，获得
radically,根本上、彻底地、以激进的方式
",
"2":"
A positive result from the first classifier triggers the evaluation of a second
classifier which has also been adjusted to achieve very high detection rates
",
"3":"
As such, the cascade attempts to reject as many negatives as possible at the
earliest stage possible. While a positive instance will trigger the evaluation
of every classifier in the cascade, this is an exceedingly rare event.
"
}
